# Provider System - PydanticAI v2 Integration ğŸ¤–

## ğŸ¯ Ãœbersicht

LinkoWiki verwendet jetzt ein **flexibles Provider-System** fÃ¼r AI-Modelle, vollstÃ¤ndig konform mit **PydanticAI v2**.

### Hauptfeatures:

âœ… **Multi-Provider Support** - OpenAI, Anthropic Claude, mehr...  
âœ… **Reasoning & Non-Reasoning** - Korrekte API-Nutzung je Typ  
âœ… **Runtime Model Switch** - Modell in laufender Session wechseln  
âœ… **JSON-Config** - Alle Provider zentral in `etc/providers.json`  
âœ… **Validation** - Verhindert falsche Settings (temperature bei Reasoning)  
âœ… **Session-Based** - Jede Session hat eigenes aktives Modell  

---

## ğŸ“ Architektur

```
tools/ai/
â”œâ”€â”€ providers.py        # Provider Registry & Validation
â”œâ”€â”€ agent_factory.py    # Agent Creation Factory
â””â”€â”€ assistant.py        # High-Level AI Interface

etc/
â””â”€â”€ providers.json      # Provider Configuration
```

### Datenfluss:

```
Session â†’ Provider ID â†’ Agent Factory â†’ Configured Agent â†’ AI Response
```

---

## ğŸ”§ Provider-Konfiguration

### `etc/providers.json`

```json
{
  "providers": [
    {
      "id": "openai-gpt4o",
      "provider": "openai",
      "model": "gpt-4o",
      "api_base": "https://api.openai.com/v1",
      "reasoning": false,
      "env_key": "OPENAI_API_KEY",
      "default_settings": {
        "temperature": 0.3
      },
      "description": "GPT-4o - Schnell und effizient"
    },
    {
      "id": "openai-o1",
      "provider": "openai",
      "model": "o1",
      "reasoning": true,
      "env_key": "OPENAI_API_KEY",
      "default_settings": {
        "reasoning_effort": "medium"
      },
      "description": "O1 - Reasoning fÃ¼r komplexe Probleme"
    }
  ],
  "default_provider": "openai-gpt4o"
}
```

### Provider-Felder:

| Feld | Beschreibung | Erforderlich |
|------|--------------|--------------|
| `id` | Eindeutige Provider-ID | âœ“ |
| `provider` | Provider-Name (openai, anthropic) | âœ“ |
| `model` | Modellname | âœ“ |
| `api_base` | API-Basis-URL | âœ— |
| `reasoning` | Reasoning-Modell? (true/false) | âœ“ |
| `env_key` | Environment-Variable fÃ¼r API-Key | âœ“ |
| `default_settings` | Standard Model-Settings | âœ“ |
| `description` | Beschreibung | âœ— |

---

## ğŸš¨ Validierungsregeln

### Reasoning-Modelle (`reasoning: true`)

âœ… **Erlaubt:**
- `reasoning_effort: low|medium|high`

âŒ **NICHT erlaubt:**
- `temperature`
- `top_p`

### Non-Reasoning-Modelle (`reasoning: false`)

âœ… **Erlaubt:**
- `temperature`
- `top_p`
- Andere Standard-Parameter

âŒ **NICHT erlaubt:**
- `reasoning_effort`

**Fehler bei VerstoÃŸ:**
```
ValueError: Reasoning model 'openai-o1' does not support temperature/top_p.
Use reasoning_effort instead.
```

---

## ğŸ’» Verwendung

### In der Session Shell

```bash
# Session starten
linkowiki-admin session start -w

# Session Shell Ã¶ffnen
linkowiki-admin session shell

# Aktuelles Modell anzeigen
:model

# VerfÃ¼gbare Modelle auflisten
:model list

# Modell wechseln
:model set openai-o1

# Jetzt mit neuem Modell arbeiten
erstelle ein wiki fÃ¼r docker
```

### Modell-Liste Beispiel:

```
ğŸ¤– VerfÃ¼gbare Modelle
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â†’ openai-gpt4o 
  GPT-4o - Schnell und effizient fÃ¼r allgemeine Aufgaben

  openai-gpt4o-mini 
  GPT-4o Mini - KostengÃ¼nstig fÃ¼r einfache Tasks

  openai-o1 [R]
  O1 - Reasoning-Modell fÃ¼r komplexe ProblemlÃ¶sung

  openai-o1-mini [R]
  O1 Mini - Schnelles Reasoning fÃ¼r mittlere KomplexitÃ¤t

  anthropic-claude-3-5-sonnet 
  Claude 3.5 Sonnet - Balanciert und vielseitig

Tippe ':model set <id>' zum Wechseln
```

**Legende:**
- `â†’` = Aktives Modell
- `[R]` = Reasoning-Modell

---

## ğŸ”Œ API-Integration

### Environment Variables

Setze API-Keys in deiner Shell:

```bash
export OPENAI_API_KEY="sk-..."
export ANTHROPIC_API_KEY="sk-ant-..."
```

Oder in `.env` Datei:

```env
OPENAI_API_KEY=sk-...
ANTHROPIC_API_KEY=sk-ant-...
```

---

## ğŸ› ï¸ Programmatische Verwendung

### Agent Factory verwenden:

```python
from tools.ai.agent_factory import AgentFactory
from tools.ai.assistant import AIResult

# Agent fÃ¼r spezifischen Provider erstellen
agent = AgentFactory.create_agent(
    provider_id="openai-o1",
    output_type=AIResult,
    system_prompt="Du bist ein Assistent...",
    custom_settings={"reasoning_effort": "high"}  # Optional override
)

# Agent verwenden
result = agent.run_sync("Was ist Docker?")
print(result.output.message)
```

### Session-basierte Agent-Erstellung:

```python
from tools.ai.agent_factory import create_agent_for_session
from tools.session.manager import load_session

# LÃ¤dt Provider-ID aus Session
session = load_session()

agent = create_agent_for_session(
    session=session,
    output_type=AIResult,
    system_prompt="..."
)
```

---

## ğŸ”„ Provider wechseln

### In laufender Session:

```python
from tools.session.manager import set_active_provider

# Modell wechseln
set_active_provider("openai-o1")

# Neue Requests nutzen jetzt O1
```

### Validierung:

Das System validiert automatisch:
- Provider existiert
- API-Key verfÃ¼gbar
- Settings passen zum Modell-Typ

---

## ğŸ“Š Session State

Neue Session-Felder:

```json
{
  "id": "2026-01-01T12:00:00",
  "active_provider_id": "openai-gpt4o",
  "write": true,
  ...
}
```

`active_provider_id` wird:
- Beim Session-Start auf `default_provider` gesetzt
- Bei `:model set` aktualisiert
- FÃ¼r jeden AI-Request verwendet

---

## ğŸ§ª Beispiele

### Reasoning-Modell fÃ¼r komplexe Aufgabe:

```bash
:model set openai-o1

erstelle einen kompletten security-guide fÃ¼r kubernetes mit best practices
```

O1 nutzt `reasoning_effort: medium` und denkt strukturiert durch das Problem.

### Non-Reasoning fÃ¼r schnelle Antwort:

```bash
:model set openai-gpt4o-mini

was ist der unterschied zwischen docker und podman?
```

GPT-4o Mini antwortet schnell mit `temperature: 0.3`.

---

## ğŸš€ Neue Provider hinzufÃ¼gen

1. **`etc/providers.json` editieren:**

```json
{
  "id": "anthropic-claude-opus",
  "provider": "anthropic",
  "model": "claude-3-opus-20240229",
  "reasoning": false,
  "env_key": "ANTHROPIC_API_KEY",
  "default_settings": {
    "temperature": 0.3
  },
  "description": "Claude Opus - HÃ¶chste QualitÃ¤t"
}
```

2. **API-Key setzen:**

```bash
export ANTHROPIC_API_KEY="sk-ant-..."
```

3. **Sofort verfÃ¼gbar:**

```bash
:model list
:model set anthropic-claude-opus
```

---

## âš ï¸ Fehlerbehandlung

### HÃ¤ufige Fehler:

**1. API-Key fehlt:**
```
ValueError: API key not found. Set environment variable: OPENAI_API_KEY
```

**LÃ¶sung:**
```bash
export OPENAI_API_KEY="sk-..."
```

**2. Falscher Provider:**
```
ValueError: Unknown provider: wrong-id
```

**LÃ¶sung:**
```bash
:model list  # Zeigt verfÃ¼gbare Provider
```

**3. Falsche Settings:**
```
ValueError: Reasoning model 'openai-o1' does not support temperature/top_p
```

**LÃ¶sung:** Settings in `providers.json` korrigieren (nur `reasoning_effort` bei Reasoning-Modellen).

---

## ğŸ”® Roadmap

- [ ] **Custom Provider** - Eigene OpenAI-kompatible Endpoints
- [ ] **Model Presets** - Gespeicherte Setting-Profile
- [ ] **Cost Tracking** - Token-Usage pro Provider
- [ ] **Fallback Chain** - Automatischer Fallback bei Errors
- [ ] **Local Models** - Ollama-Integration
- [ ] **Multi-Model Responses** - Vergleich mehrerer Modelle

---

## ğŸ“– Weitere Docs

- [PydanticAI Documentation](https://ai.pydantic.dev/)
- [OpenAI Models](https://platform.openai.com/docs/models)
- [Anthropic Claude](https://docs.anthropic.com/en/docs/models-overview)

---

**Version:** 3.0  
**PydanticAI:** v2.x  
**Datum:** 2026-01-01
